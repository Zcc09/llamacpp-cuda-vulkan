services:
  rpc-amd:
    image: ghcr.io/zcc09/llamacpp-cuda-vulkan:latest
    container_name: llama-rpc-amd
    pull_policy: missing
    restart: unless-stopped
    init: true
    devices:
      - /dev/dri:/dev/dri
      - /dev/kfd:/dev/kfd
    environment:
      - LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu
    command: >
      /usr/local/bin/vulkan/rpc-server 
      --host 0.0.0.0 
      --port ${PORT_RPC:-50052}
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "${PORT_RPC:-50052}"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 30s

  llama-main:
    image: ghcr.io/zcc09/llamacpp-cuda-vulkan:latest
    container_name: llama-main
    pull_policy: missing
    restart: unless-stopped
    ports:
      - "${PORT_MAIN:-30069}:8080"
    volumes:
      - "${HOST_MODEL_DIR}:/models"
    depends_on:
      rpc-amd:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    # -------------------------------------------------------
    # CRITICAL CHANGE: Removed -fa, -ctk, -ctv
    # We are going back to basics to stop the crash.
    # -------------------------------------------------------
    command: >
      /usr/local/bin/cuda/llama-server 
      -m /models/${MODEL_FILE}
      --host 0.0.0.0 
      --port 8080 
      --rpc rpc-amd:${PORT_RPC:-50052}
      -ngl 99
      --mmap
      -c ${CTX_SIZE:-65536}
      -b ${BATCH_SIZE:-2048}
      -ub ${UBATCH_SIZE:-512}
      -t ${THREADS:-6}
      --timeout 0
