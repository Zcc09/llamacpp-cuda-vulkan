services:
  llamacpp:
    image: ghcr.io/zcc09/llamacpp-cuda-vulkan:latest
    pull_policy: always
    container_name: llamacpp
    restart: unless-stopped
    ports:
      - "30069:8080"
    volumes:
      - "/mnt/Fast/Apps/llamacpp/models:/models"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility,video,display
    devices:
      - /dev/dri:/dev/dri
      - /dev/kfd:/dev/kfd 
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
