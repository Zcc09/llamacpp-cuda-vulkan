services:
  llamacpp:
    image: ghcr.io/zcc09/llamacpp-cuda-vulkan:latest
    pull_policy: always
    container_name: llamacpp
    restart: unless-stopped
    ports:
      - "30069:8080"
    volumes:
      - "/mnt/Fast/Apps/llamacpp:/models"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility,video,display
      - GGML_CUDA_GRAPH_OPT=1
    devices:
      - /dev/dri:/dev/dri # For Intel iGPU and AMD GPU (Vulkan)
      - /dev/kfd:/dev/kfd # Specifically for AMD GPU compute
    command:
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8080"
      - "--models-dir"
      - "/models"
      - "--n-gpu-layers"
      - "99"
      - "--split-mode"
      - "layer"
      - "--device"
      - "cuda0,cuda1,vulkan0"
      - "-c"
      - "32000"
      - "-ctk"
      - "q8_0"
      - "-ctv"
      - "q8_0"
      - "-fa"
      - "auto"
      - "-b"
      - "2048"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
