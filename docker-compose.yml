services:
  amd-worker:
    # REPLACEME: with your username in lowercase
    image: ghcr.io/zcc09/llama-rocm:latest
    container_name: llama-amd-worker
    devices:
      - /dev/dri:/dev/dri
      - /dev/kfd:/dev/kfd
    command: ["-H", "0.0.0.0", "-p", "50052"]
    restart: always

  llama-server:
    # REPLACEME: with your username in lowercase
    image: ghcr.io/zcc09/llama-cuda:latest
    container_name: llama-main-server
    depends_on:
      - amd-worker
    ports:
      - "8080:8080"
    volumes:
      - "/mnt/Fast/Apps/llamacpp/models:/models"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: all
    command: >
      -m /models/nemotron-3-nano-30b-q4km.gguf
      --host 0.0.0.0
      --port 8080
      --n-gpu-layers 999
      --main-gpu 0
      --tensor-split 16,12
      --rpc amd-worker:50052
      --fa on
