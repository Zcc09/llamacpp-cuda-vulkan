ARG CUDA_VERSION=12.4.1
FROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS builder

ARG LLAMA_TAG=master
WORKDIR /app

RUN apt-get update && apt-get install -y git build-essential cmake curl jq

RUN git clone --depth 1 --branch ${LLAMA_TAG} https://github.com/ggerganov/llama.cpp.git . || \
    git clone --depth 1 --branch master https://github.com/ggerganov/llama.cpp.git .

RUN sed -i '1i#include <cstddef>' ./llama.cpp/common/ngram-mod.h

ENV LIBRARY_PATH="/usr/local/cuda/lib64/stubs:${LIBRARY_PATH}"

RUN cmake -B build \
    -DGGML_CUDA=ON \
    -DCMAKE_CUDA_ARCHITECTURES="86;89" \
    -DCMAKE_BUILD_TYPE=Release \
    -DCMAKE_LIBRARY_PATH=/usr/local/cuda/lib64/stubs \
    -DCMAKE_EXE_LINKER_FLAGS="-Wl,--allow-shlib-undefined"
    
# Build ALL components: common, mtmd, and llama-server
RUN cmake --build build --config Release --target common mtmd llama-server -j $(nproc)

# --- RUNTIME STAGE ---
FROM nvidia/cuda:${CUDA_VERSION}-runtime-ubuntu22.04
WORKDIR /app
# Copy the server executable
COPY --from=builder /app/build/bin/llama-server /app/llama-server
# *** NEW: Copy the missing library ***
COPY --from=builder /app/build/bin/libmtmd.so.0 /app/libmtmd.so.0 
ENV LD_LIBRARY_PATH /app:${LD_LIBRARY_PATH}
EXPOSE 8080
ENTRYPOINT ["./llama-server"]
